<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Nazmul Karim</title>
  
  <meta name="author" content="Nazmul Karim">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
<link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
  <link rel   = "stylesheet" href    ="https://use.fontawesome.com/releases/v5.0.7/css/all.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Nazmul Karim</name>
              </p>
              <p>
		I am a Research Scientist at Bosch, working on the foundation model for sensor and wi-fi data interpretation. I recently completed my Ph.D. journey at the <a href="https://lcwnlab.eecs.ucf.edu/">LCWN Lab, UCF</a>, where I was advised by <a href="https://www.ece.ucf.edu/person/nazanin-rahnavard/">Prof. Nazanin Rahnavard</a> and co-advised by <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Prof. Mubarak Shah</a>. I have a broad interest in various topics in computer vision and machine learning. My Ph.D. research primarily focused on safe, responsible, and robust AI; including adversarial attacks and defenses, out-of-distribution (OOD) robustness, domain adaptation, and learning with noisy labels. I have also worked on compressive sensing, 3D Scene generation, video generation, continual learning, and multi-modal learning. So far, I have published <strong>8 first-authored top-tier conference/journal papers</strong>. 	
<!-- 		   I am a Postdoctoral Scientist at Amazon Search Science & AI, working at the intersection of video understanding and large language models. I recently completed my Ph.D. journey at the <a href="https://www.crcv.ucf.edu/">Center for Research in Computer Vision, UCF</a>, where I was advised by <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Prof. Mubarak Shah</a>. I have a broad interest in various topics in computer vision and machine learning. My Ph.D. research primarily focused on learning with limited labels, including semi-supervised learning, few-shot learning, and self-supervised learning. I have also worked on activity detection, temporal action localization, learning with noisy labels, and multi-modal learning.    -->
<!-- 		      I am a Postdoctoral Scientist at Amazon Search Science & AI working at the intersection of video understanding and large language models. I recently wrapped up my Ph.D. journey at <a href="https://www.crcv.ucf.edu/">Center for Research in Computer Vision, UCF</a>, where I am advised by <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Prof. Mubarak Shah</a>. I am broadly interested interested in computer vision, and machine learning. My PhD research primarily focused on learning with limited labels, which mostly includes semi-supervised learning, few-shot learning, and self-supervised learning. I have also worked on activity detection, temporal action localization, learning with noisy labels, and multi-modal learning. -->
              </p>
              <p style="text-align:center">
                <a href="mailto:nazmul.karim170@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/Resume_Nazmul_C.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=mgi3sEgAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/nazmul170">Twitter</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/nazmul-karim-1b5805115/">LinkedIn</a> &nbsp/&nbsp
                <a href="https://github.com/nazmul-karim170">GitHub</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/rsz_photo_nazmul.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/rsz_photo_nazmul.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
	
	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Updates</heading>
              <p>
		&nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> July 2024: My work on AI security got accepted to <strong>ACM CCS 2024</strong><br>
		&nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> July 2024: My work on AI security got accepted to <strong>ECCV 2024</strong><br>
		&nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> July 2024: <a href="https://latenteditor.github.io/">LatentEditor</a> got accepted to <strong>ECCV 2024</strong><br>
		&nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> July 2024: <a href="https://free-editor.github.io/">Free-Editor</a> got accepted to <strong>ECCV 2024</strong><br>
		&nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> February 2024: Joined <strong>Bosch</strong> as a Research Scientist<br>
		&nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> December 2023: Joined UCF CRCV as a Postdoctoral Scientist<br>
		&nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> November 2023: Successfully defended my PhD dissertation<br>
		&nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> May 2023: Started summer internship at <strong>Amazon Web Services</strong><br>
		&nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> February 2023: My work on source free domain adaptation got accepted to <strong>CVPR 2023</strong><br>
		&nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> May 2022: Started summer internship at SRI International<br>
		&nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> April 2022: Paper accepted to SPIE 2022<br>
		&nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> February 2022: <strong>Three papers</strong> accepted to <strong>CVPR 2022</strong><br>
                &nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> October 2021: Paper accepted to <strong>IEEE Transaction</strong> on Forensics and Information Security (TIFS)<br>
		&nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> May 2020: Completed MS in Computer Engineering<br>
		&nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> June 2019: Paper accepted to <strong>IEEE MLSP 2019</strong><br>
<!-- 		&nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> March 2021: Paper accepted to CVPR 2021<br>
		&nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> January 2021: Paper accepted to ICLR 2021<br> -->
<!--                 &nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> May 2021: Started summer internship with the Perception team at Aurora<br>
                &nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> March 2021: Paper accepted at CVPR 2021<br>
		      &nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> July 2022: Our realistic semi-supervised learning paper has been selected for <font color="red">Oral</font> presentation at ECCV 2022<br>
                &nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> January 2021: Paper accepted at ICLR 2021<br>
		&nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> January 2021: Our Gabriella paper has been awarded the <font color="red">best scientific paper</font> award at ICPR 2020<br>
		&nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> October 2020: Paper accepted at ICPR 2020<br>
	        &nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> June 2020: Placed first at ActEV SDL Challenge (ActivityNet workshop at CVPR 2020)<br>
		&nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> October 2019: Placed second at the TRECVID leaderboard<br>
		&nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> August 2018: Started PhD at CRCV, UCF<br>  -->
              </p>
            </td>
          </tr>
        </tbody></table>
	      
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
             <tr>
             <td style="padding:20px;width:100%;vertical-align:middle"> 
               <heading>Publications</heading>
            </td>
        </tr> 
    </tbody></table>  

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:10px;width:25%;vertical-align:top">
              <img src="images/FreeEditor_eccv2024.png" alt="freeeditor-eccv2024" width="160" style="border-style: none">
            </td>
            <td style="padding-bottom:35px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2312.13663" id="MCG_journal">
                <papertitle>Free-Editor: Zero-shot Text-driven 3D Scene Editing</papertitle>
              </a>
              <br>
              <a  href="https://www.linkedin.com/in/umarkhalidai/">Umar Khalid*</a>, <a href="https://www.linkedin.com/in/hasaniqbal1292/">Hasan Iqbal*</a>, <strong>Nazmul Karim*</strong>,  <a href="https://www.linkedin.com/in/tayyab454/">Muahammad Tayyab</a>  <a href="https://www.linkedin.com/in/jing-hua-06a3956/">Jing Hua</a>, <a href="https://www.crcv.ucf.edu/chenchen/">Chen Chen</a>
              <br>
              <em>ECCV</em>, 2024
              <br>
              <a href="https://arxiv.org/pdf/2312.13663">arxiv</a> /
              <a href="data/freeEdit_eccv2024.bib">bibtex</a> /
              <a href="https://free-editor.github.io/">project</a> /
	      <a href="https://github.com/nazmul-karim170/FreeEditor-Text-to-3D-Scene-Editing">code</a>
              <p>
we propose a novel training-free 3D scene editing technique, FREE-EDITOR, which allows users to edit 3D scenes without further re-training the model during test time. Our proposed method successfully avoids the multi- view style inconsistency issue in SOTA methods with the help of a “single-view editing” scheme. Specifically, we show that editing a particular 3D scene can be performed by only modifying a single view.
	</p>
            </td>
          </tr>	
		
          <tr>
            <td style="padding:10px;width:25%;vertical-align:top">
              <img src="images/LatentEditor_eccv2024.png" alt="latent-eccv2024" width="160" style="border-style: none">
            </td>
            <td style="padding-bottom:35px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2312.09313" id="MCG_journal">
                <papertitle>LatentEditor: Text Driven Local Editing of 3D Scenes</papertitle>
              </a>
              <br>
              <a  href="https://www.linkedin.com/in/umarkhalidai/">Umar Khalid*</a>, <a href="https://www.linkedin.com/in/hasaniqbal1292/">Hasan Iqbal*</a>, <strong>Nazmul Karim*</strong>,  <a href="https://www.linkedin.com/in/tayyab454/">Muahammad Tayyab</a>  <a href="https://www.linkedin.com/in/jing-hua-06a3956/">Jing Hua</a>, <a href="https://www.crcv.ucf.edu/chenchen/">Chen Chen</a>
              <br>
              <em>ECCV</em>, 2024
              <br>
              <a href="https://arxiv.org/pdf/2312.09313">arxiv</a> /
              <a href="data/LatEdit_eccv.bib">bibtex</a> /
              <a href="https://latenteditor.github.io/">project</a> /
	      <a href="https://github.com/umarkhalidAI/LatentEditor">code</a>
              <p>
		 In this paper, we introduce LATENTEDITOR, an innovative
		framework designed to empower users with the ability to
		perform precise and locally controlled editing of neural
		fields using text prompts. Leveraging denoising diffusion
		models, we successfully embed real-world scenes into the
		latent space, resulting in a faster and more adaptable NeRF
		backbone for editing compared to traditional methods.
	</p>
            </td>
          </tr>	

	<tr>
            <td style="padding:10px;width:25%;vertical-align:top">
              <img src="images/csfda_cvpr2023.png" alt="csfda-cvpr2023" width="160" style="border-style: none">
            </td>
            <td style="padding-bottom:35px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Karim_C-SFDA_A_Curriculum_Learning_Aided_Self-Training_Framework_for_Efficient_Source_CVPR_2023_paper.pdf" id="MCG_journal">
                <papertitle>C-SFDA: A Curriculum Learning Aided Self-Training Framework for Efficient Source Free Domain Adaptation</papertitle>
              </a>
              <br>
              <strong>Nazmul Karim</strong>, <a href="https://niluthpol.github.io/">Niluthpool Mithun Chowdhury</a>, <a href="https://www.linkedin.com/in/ye-yu">Abhinav Rajvanshi</a>, <a href="https://www.linkedin.com/in/matthew-hall-35525949">Han-pang Chiu</a>, <a href="">Supun Samarasekera</a>, <a href="">Nazanin Rahnavard</a>
              <br>
              <em>CVPR</em>, 2023
              <br>
              <a href=https://openaccess.thecvf.com/content/CVPR2023/papers/Karim_C-SFDA_A_Curriculum_Learning_Aided_Self-Training_Framework_for_Efficient_Source_CVPR_2023_paper.pdf">paper</a>/
	      <a href=https://github.com/nazmul-karim170/C-SFDA_Source-Free-Domain-Adaptation">code</a>/
              <a href="data/csfda_cvpr2023.bib">bibtex</a> /
              <a href="https://www.youtube.com/watch?v=tA-8U4rD32I">video</a>
              <p>
		   A curriculum learning-aided self-training framework for SFDA that adapts efficiently and reliably to changes across domains based on selective pseudo-labeling. Specifically, we employ a curriculum learning scheme to promote learning from a restricted amount of pseudo labels selected based on their reliabilities. This simple yet effective step successfully prevents label noise propagation during different stages of adaptation and eliminates the need for costly memory-bank based label refinement.
		</p>
            </td>
          </tr>

		
          <tr>
            <td style="padding:10px;width:25%;vertical-align:top">
              <img src="images/unicon_cvpr2022.png" alt="unicon-cvpr2021" width="160" style="border-style: none">
            </td>
            <td style="padding-bottom:35px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2203.14542" id="MCG_journal">
                <papertitle>UNICON: Combating Label Noise Through Uniform Selection and Contrastive Learning</papertitle>
              </a>
              <br>
              <strong>Nazmul Karim</strong>, <a href="https://www.linkedin.com/in/mamshad-nayeem-rizve/">Mamshad Nayeem Rizve</a>, <a href="https://www.ece.ucf.edu/person/nazanin-rahnavard/">Nazanin Rahnavard</a>, <a href="https://ajmalsaeed.net/">Ajmal Mian</a>, <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>
              <br>
              <em>CVPR</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2203.14542">arxiv</a> /
              <a href="data/unicon_cvpr2022.bib">bibtex</a> /
              <a href="https://github.com/nazmul-karim170/unicon-noisy-label">code</a>
              <p>
		UNICON is a robust sample selection approach for training with high label noise. It incorporates a Jensen-Shannon divergence-based uniform sample selection mechanism and contrastive learning.
	</p>
            </td>
          </tr>
		
          <tr>
            <td style="padding:10px;width:25%;vertical-align:top">
              <img src="images/cnll_cvpr2022.png" alt="cnll-cvpr2021" width="160" style="border-style: none">
            </td>
            <td style="padding-bottom:35px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2022W/CLVision/papers/Karim_CNLL_A_Semi-Supervised_Approach_for_Continual_Noisy_Label_Learning_CVPRW_2022_paper.pdf" id="MCG_journal">
                <papertitle>CNLL: A Semi-supervised Approach For Continual Noisy Label Learning</papertitle>
              </a>
              <br>
              <strong>Nazmul Karim</strong>, <a href="https://www.linkedin.com/in/umarkhalidai/">Umar Khalid</a>, <a href="https://www.linkedin.com/in/ashkan-esmaeili-19391118a/">Ashkan Esmaeili</a>, <a href="https://www.ece.ucf.edu/person/nazanin-rahnavard/">Nazanin Rahnavard</a>
              <br>
              <em>CVPR</em>, 2022
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2022W/CLVision/papers/Karim_CNLL_A_Semi-Supervised_Approach_for_Continual_Noisy_Label_Learning_CVPRW_2022_paper.pdf">arxiv</a> /
              <a href="data/cnll_cvpr2022.bib">bibtex</a> /
              <a href="https://github.com/nazmul-karim170/CNLL-Continual_Learning_Noisy_Labels">code</a>
              <p>
		The task of continual learning requires careful design of algorithms that can tackle catastrophic forgetting. However, the noisy label, which is inevitable in a real-world
scenario, seems to exacerbate the situation. While very few
studies have addressed the issue of continual learning under
noisy labels, long training time and complicated training schemes limit their applications in most cases. In contrast, we propose a simple purification technique to effectively cleanse the online data stream that is both cost-effective and more accurate.
	</p>
            </td>
          </tr>	
		
          <tr>
            <td style="padding:10px;width:25%;vertical-align:top">
              <img src="images/rodd_cvpr2022.png" alt="rodd-cvpr2021" width="160" style="border-style: none">
            </td>
            <td style="padding-bottom:35px;width:75%;vertical-align:middle">
              <a href=https://arxiv.org/pdf/2204.02553" id="MCG_journal">
                <papertitle>RODD: A Self-Supervised Approach for Robust Out-of-Distribution Detection</papertitle>
              </a>
              <br>
              <a href="https://www.linkedin.com/in/umarkhalidai/">Umar Khalid</a>, <a href="https://www.linkedin.com/in/ashkan-esmaeili-19391118a/">Ashkan Esmaeili</a>, <strong>Nazmul Karim</strong>, <a href="https://www.ece.ucf.edu/person/nazanin-rahnavard/">Nazanin Rahnavard</a>
              <br>
              <em>CVPR</em>, 2022
              <br>
              <a href="https://arxiv.org/pdf/2204.02553">arxiv</a> /
              <a href="data/rodd_cvpr2022.bib">bibtex</a> /
              <a href="https://github.com/nazmul-karim170/RODD">code</a>
              <p>
		We propose a simple yet effective generalized OOD detection method independent of out-of-distribution datasets. Our approach relies on self-supervised feature learning of the training samples, where the embeddings lie on a compact low-dimensional space. Motivated by the recent studies that show self-supervised adversarial contrastive learning helps robustify the model, we empirically show that a pre-trained model with self-supervised contrastive learning yields a better model for uni-dimensional
		feature learning in the latent space.
	</p>
            </td>
          </tr>	

          <tr>
            <td style="padding:10px;width:25%;vertical-align:top">
              <img src="images/rf_spie2022.png" alt="rf-spie2022" width="160" style="border-style: none">
            </td>
            <td style="padding-bottom:35px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2204.03564" id="MCG_journal">
                <papertitle>RF Signal Transformation and Classification using Deep Neural Networks</papertitle>
              </a>
              <br>
               <a href="https://www.linkedin.com/in/umarkhalidai/">Umar Khalid</a>, <strong>Nazmul Karim</strong>, <a href="https://www.ece.ucf.edu/person/nazanin-rahnavard/">Nazanin Rahnavard</a>
              <br>
              <em>SPIE </em>, 2022
              <br>
              <a href="https://arxiv.org/pdf/2204.03564">arxiv</a> /
              <a href="data/rf_spie2022.bib">bibtex</a> /
              <a href="https://github.com/umarkhalidAI/RF_Classification">code</a>
              <p>
		Deep neural networks (DNNs) designed for computer vision and natural language processing tasks cannot be
		directly applied to the radio frequency (RF) datasets. To address this challenge, we propose to convert the
		raw RF data to data types that are suitable for off-the-shelf DNNs by introducing a convolutional transform
		technique.
		</p>
            </td>
          </tr>	
		
          <tr>
            <td style="padding:10px;width:25%;vertical-align:top">
              <img src="images/odyssey_tifs2021.png" alt="odyssey-tifs2021" width="160" style="border-style: none">
            </td>
            <td style="padding-bottom:35px;width:75%;vertical-align:middle">
              <a href=https://arxiv.org/pdf/2007.08142" id="MCG_journal">
                <papertitle>Odyssey: Creation, Analysis, and Detection of Trojan Models</papertitle>
              </a>
              <br>
              <a href="https://www.linkedin.com/in/marzieh-edraki-58a4371a1/">Marzieh Edraki*/a>, <strong>Nazmul Karim*</strong>, <a href="https://www.ece.ucf.edu/person/nazanin-rahnavard/">Nazanin Rahnavard</a>, <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>
              <br>
              <em>IEEE Transactions on Information Forensics and Security </em>, 2021
              <br>
              <a href="https://arxiv.org/pdf/2007.08142">arxiv</a> /
              <a href="data/odyssey_tifs.bib">bibtex</a> /
              <a href="https://www.crcv.ucf.edu/research/projects/odyssey-creation-analysis-and-detection-of-trojan-models/">dataset</a> /
              <a href="https://github.com/LCWN-Lab/Odyssey">code</a>
              <p>
		We propose a detector that is based on the analysis
		of the intrinsic DNN properties; that are affected due to the
		Trojaning process. For a comprehensive analysis, we develop Odysseus1
		, the most diverse dataset to date with over
		3,000 clean and Trojan models. Odysseus covers a large
		spectrum of attacks; generated by leveraging the versatility
		in trigger designs and source to target class mappings. Our
		analysis results show that Trojan attacks affect the classifier
		margin and shape of decision boundary around the manifold of clean data. Exploiting these two factors, we propose an efficient Trojan detector that operates without any
		knowledge of the attack and significantly outperforms existing methods. 
		</p>
            </td>
          </tr>	        

          <tr>
            <td style="padding:10px;width:25%;vertical-align:top">
              <img src="images/rlncs_mlsp2021.png" alt="rlncs-mlsp2021" width="160" style="border-style: none">
            </td>
            <td style="padding-bottom:35px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2107.00838" id="MCG_journal">
                <papertitle>Rl-ncs: Reinforcement learning-based data-driven approach for nonuniform compressed sensing</papertitle>
              </a>
              <br>
               <strong>Nazmul Karim*</strong>, <a href="https://zaeemzadeh.com/">Alireza Zaeemzadeh/a>, <a href="https://www.ece.ucf.edu/person/nazanin-rahnavard/">Nazanin Rahnavard</a>
              <br>
              <em>IEEE MLSP </em>, 2019
              <br>
              <a href="https://arxiv.org/pdf/2107.00838">arxiv</a> /
              <a href="data/rlncs_mlsp2019.bib">bibtex</a> /
              <a href="https://github.com/nazmul-karim170/RL-NCS">code</a>
              <p>
		A reinforcement-learning-based non-uniform compressed sensing
		(NCS) framework for time-varying signals is introduced. The proposed scheme, referred to as RL-NCS, aims to boost the performance
		of signal recovery through an optimal and adaptive distribution of sensing energy among two groups of coefficients of the signal, referred to as region of interest (ROI) coefficients and non-ROI coefficients. The coefficients in ROI usually have greater importance
		and need to be reconstructed with higher accuracy compared to non-ROI coefficients.
		</p>
            </td>
          </tr>	


  </table>
</body>

</html>
