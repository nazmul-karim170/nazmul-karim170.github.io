<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Mamshad Nayeem Rizve</title>
  
  <meta name="author" content="Mamshad Nayeem Rizve">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
<link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
  <link rel   = "stylesheet" href    ="https://use.fontawesome.com/releases/v5.0.7/css/all.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Mamshad Nayeem Rizve</name>
              </p>
              <p>
		   I am a Postdoctoral Scientist at Amazon Search Science & AI, working at the intersection of video understanding and large language models. I recently completed my Ph.D. journey at the <a href="https://www.crcv.ucf.edu/">Center for Research in Computer Vision, UCF</a>, where I was advised by <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Prof. Mubarak Shah</a>. I have a broad interest in various topics in computer vision and machine learning. My Ph.D. research primarily focused on learning with limited labels, including semi-supervised learning, few-shot learning, and self-supervised learning. I have also worked on activity detection, temporal action localization, learning with noisy labels, and multi-modal learning.   
<!-- 		      I am a Postdoctoral Scientist at Amazon Search Science & AI working at the intersection of video understanding and large language models. I recently wrapped up my Ph.D. journey at <a href="https://www.crcv.ucf.edu/">Center for Research in Computer Vision, UCF</a>, where I am advised by <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Prof. Mubarak Shah</a>. I am broadly interested interested in computer vision, and machine learning. My PhD research primarily focused on learning with limited labels, which mostly includes semi-supervised learning, few-shot learning, and self-supervised learning. I have also worked on activity detection, temporal action localization, learning with noisy labels, and multi-modal learning. -->
              </p>
              <p style="text-align:center">
                <a href="mailto:nayeemrizve@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/CV_Mamshad.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=kA8ZM5oAAAAJ&hl">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/MamshadR">Twitter</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/mamshad-nayeem-rizve/">LinkedIn</a> &nbsp/&nbsp
                <a href="https://github.com/nayeemrizve">GitHub</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/rizve_circle.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/rizve_circle.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
	
	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Updates</heading>
              <p>
		&nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> February 2024: Paper accepted to CVPR 2024<br>
		&nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> January 2024: <a href="https://arxiv.org/abs/2310.08584">DoRA</a> got accepted to ICLR 2024 as an <font color="red">Oral</font><br>
		&nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> August 2023: Joined Amazon Search Science & AI as a Postdoctoral Scientist<br>
		&nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> July 2023: Three papers accepted to ICCV 2023<br>
		&nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> July 2023: Successfully defended my PhD dissertation<br>
		&nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> February 2023: Two papers accepted to CVPR 2023<br>
		&nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> October 2022: Patent granted for real-time spatio-temporal activity detection from untrimmed videos<br>
		&nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> July 2022: Two papers accepted to ECCV 2022; <a href="https://arxiv.org/abs/2207.02269">TRSSL</a> accepted as an <font color="red">Oral</font><br>
                &nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> May 2022: Started summer internship at Microsoft<br>
                &nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> March 2022: Paper accepted to CVPR 2022<br>
		&nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> May 2021: Started summer internship at Aurora<br>
		&nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> March 2021: Paper accepted to CVPR 2021<br>
		&nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> January 2021: Paper accepted to ICLR 2021<br>
<!--                 &nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> May 2021: Started summer internship with the Perception team at Aurora<br>
                &nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> March 2021: Paper accepted at CVPR 2021<br>
		      &nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> July 2022: Our realistic semi-supervised learning paper has been selected for <font color="red">Oral</font> presentation at ECCV 2022<br>
                &nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> January 2021: Paper accepted at ICLR 2021<br>
		&nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> January 2021: Our Gabriella paper has been awarded the <font color="red">best scientific paper</font> award at ICPR 2020<br>
		&nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> October 2020: Paper accepted at ICPR 2020<br>
	        &nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> June 2020: Placed first at ActEV SDL Challenge (ActivityNet workshop at CVPR 2020)<br>
		&nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> October 2019: Placed second at the TRECVID leaderboard<br>
		&nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> August 2018: Started PhD at CRCV, UCF<br>  -->
              </p>
            </td>
          </tr>
        </tbody></table>
	      
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
             <tr>
             <td style="padding:20px;width:100%;vertical-align:middle"> 
               <heading>Publications</heading>
            </td>
        </tr> 
    </tbody></table>  

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

	<tr>
            <td style="padding:10px;width:25%;vertical-align:top">
              <img src="images/dora_arxiv2023.png" alt="pmml-iccv2023" width="160" style="border-style: none">
            </td>
            <td style="padding-bottom:35px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2310.08584" id="MCG_journal">
                <papertitle>Is ImageNet worth 1 video? Learning strong image encoders from 1 long unlabelled video</papertitle>
              </a>
              <br>
              <a href="https://shashankvkt.github.io/">Shashanka Venkataramanan</a>, <strong>Mamshad Nayeem Rizve</strong>, <a href="https://scholar.google.co.uk/citations?user=IUZ-7_cAAAAJ&hl=en/">João Carreira</a>, <a href="https://yukimasano.github.io/">Yuki M. Asano*</a>, <a href="https://avrithis.net/">Yannis Avrithis*</a>
              <br>
              <em>ICLR</em>, 2024 <font color="red">(Oral presentation; in top 1.2%)</font>
              <br>
              <a href="https://arxiv.org/abs/2310.08584">arxiv</a> /
              <a href="https://openreview.net/forum?id=Yen1lGns2o">openreview</a> /
              <a href="https://shashankvkt.github.io/dora">project page</a> /
              <a href="https://huggingface.co/datasets/shawshankvkt/Walking_Tours">dataset</a> /
              <a href="ComingSoon">code</a> /
              <a href="data/dora_arxiv2023.bib">bibtex</a>
	      <p></p>
              <p>
		Using just “1 video” from our new egocentric dataset - Walking Tours, we develop a new method that outperforms DINO pretrained on ImageNet on image and video downstream tasks.
	      </p>
		    
            </td>
          </tr>

	<tr>
            <td style="padding:10px;width:25%;vertical-align:top">
              <img src="images/pmml_iccv2023.png" alt="pmml-iccv2023" width="160" style="border-style: none">
            </td>
            <td style="padding-bottom:35px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Swetha_Preserving_Modality_Structure_Improves_Multi-Modal_Learning_ICCV_2023_paper.pdf" id="MCG_journal">
                <papertitle>Preserving Modality Structure Improves Multi-Modal Learning</papertitle>
              </a>
              <br>
              <a href="https://swetha5.github.io/">Sirnam Swetha</a>, <strong>Mamshad Nayeem Rizve</strong>, <a href="https://ninatu.github.io/">Nina Shvetsova</a>, <a href="https://hildekuehne.github.io/">Hilde Kuehne</a>, <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>
              <br>
              <em>ICCV</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2308.13077">arxiv</a> /
              <a href="data/pmml_iccv2023.bib">bibtex</a>
	      <p></p>
              <p>
		Multi-modal self-supervised methods struggle with out-of-domain data due to ignoring the inherent modality-specific semantic structure. To address this issue, we introduce a Semantic-Structure-Preserving Consistency approach that preserves modality-specific relationships in the joint embedding space by utilizing a Multi-Assignment Sinkhorn-Knopp algorithm. This approach achieves state-of-the-art performance across various datasets and generalizes to both in-domain and out-of-domain datasets.
	      </p>
		    
            </td>
          </tr>
	<tr>
            <td style="padding:10px;width:25%;vertical-align:top">
              <img src="images/cdfslv_iccv2023.png" alt="cdfslv-iccv2023" width="160" style="border-style: none">
            </td>
            <td style="padding-bottom:35px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Samarasinghe_CDFSL-V_Cross-Domain_Few-Shot_Learning_for_Videos_ICCV_2023_paper.pdf" id="MCG_journal">
                <papertitle>CDFSL-V: Cross-Domain Few-Shot Learning for Videos</papertitle>
              </a>
              <br>
              <a href="https://www.linkedin.com/in/sarinda-samarasinghe">Sarinda Samarasinghe</a>, <strong>Mamshad Nayeem Rizve</strong>, <a href="https://www.linkedin.com/in/navid-kardan-b2630a88/">Navid Kardan</a>, <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>
              <br>
              <em>ICCV</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2309.03989">arxiv</a> /
              <a href="data/cdfslv_iccv2023.bib">bibtex</a> /
              <a href="https://github.com/Sarinda251/CDFSL-V">code</a>
	      <p></p>
              <p>
		We introduce CDFSL-V, a challenging cross-domain few-shot learning problem in the video domain. We present a carefully designed solution that uses self-supervised learning and curriculum learning to balance information from the source and target domains. Our approach employs a masked autoencoder-based self-supervised training objective to learn from both domains and transition from class discriminative features in the source data to target-domain-specific features in a progressive curriculum.
	      </p>
		    
            </td>
          </tr>
	<tr>
            <td style="padding:10px;width:25%;vertical-align:top">
              <img src="images/ssda_iccv2023.png" alt="ssda-iccv2023" width="160" style="border-style: none">
            </td>
            <td style="padding-bottom:35px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ahmed_SSDA_Secure_Source-Free_Domain_Adaptation_ICCV_2023_paper.pdf" id="MCG_journal">
                <papertitle>SSDA: Secure Source-Free Domain Adaptation</papertitle>
              </a>
              <br>
              <a href="https://sabbiracoustic1006.github.io/">Sabbir Ahmed*</a>, <a href="https://www.linkedin.com/in/abdullah-al-arafat-12a951bb/">Abdullah Al Arafat*</a>, <strong>Mamshad Nayeem Rizve*</strong>, <a href="https://www.linkedin.com/in/ramsrahim">Rahim Hossain</a>, <a href="https://www.csc.ncsu.edu/people/zguo32">Zhishan Guo</a>, <a href="https://www.adnansirajrakin.com/">Adnan Siraj Rakin</a>
              <br>
              <em>ICCV</em>, 2023
              <br>
              <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ahmed_SSDA_Secure_Source-Free_Domain_Adaptation_ICCV_2023_paper.pdf">paper</a>/
              <a href="data/ssda_iccv2023.bib">bibtex</a>/
		<a href="https://github.com/ML-Security-Research-LAB/SSDA">code</a>  
              <p>
		 We analyze the security challenges in Source-Free Domain Adaptation (SFDA), where the target domain owner lacks access to the source dataset and is unaware of the source model's training process, making it susceptible to Backdoor/Trojan attacks.  We propose secure source-free domain adaptation (SSDA), which uses model compression and knowledge transfer with a spectral-norm-based penalty to defend the target model from backdoor attacks. Our evaluations demonstrate that SSDA effectively defends against such attacks with minimal impact on test accuracy.
		</p>
            </td>
          </tr>
		
	<tr>
            <td style="padding:10px;width:25%;vertical-align:top">
              <img src="images/pivotal_cvpr2023.png" alt="pivotal-cvpr2023" width="160" style="border-style: none">
            </td>
            <td style="padding-bottom:35px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Rizve_PivoTAL_Prior-Driven_Supervision_for_Weakly-Supervised_Temporal_Action_Localization_CVPR_2023_paper.pdf" id="MCG_journal">
                <papertitle>PivoTAL: Prior-Driven Supervision for Weakly-Supervised Temporal Action Localization</papertitle>
              </a>
              <br>
              <strong>Mamshad Nayeem Rizve*</strong>, <a href="https://g1910.github.io/">Gaurav Mittal*</a>, <a href="https://www.linkedin.com/in/ye-yu">Ye Yu</a>, <a href="https://www.linkedin.com/in/matthew-hall-35525949">Matthew Hall</a>, <a href="https://www.linkedin.com/in/sandra-s-59a08298">Sandra Sajeev</a>, <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>, <a href="https://www.microsoft.com/en-us/research/people/meic/">Mei Chen</a>
              <br>
              <em>CVPR</em>, 2023
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Rizve_PivoTAL_Prior-Driven_Supervision_for_Weakly-Supervised_Temporal_Action_Localization_CVPR_2023_paper.pdf">paper</a>/
              <a href="data/pivotal_cvpr2023.bib">bibtex</a> /
              <a href="https://youtu.be/6kAoQjXfzio">video</a>
              <p>
		   PivoTAL approaches Weakly-Supervised Temporal Action Localization from localization-by-localization perspective by learning to localize the action snippets directly. To this end, PivoTAL introduces a novel algorithm that exploits the inherent spatiotemporal structure of the video data in the form of action-specific scene prior, action snippet generation prior, and learnable Gaussian prior to generate pseudo-action snippets. These pseudo-action snippets provide additional supervision, complementing the weak video-level annotations during training.
		</p>
            </td>
          </tr>
	<tr>
            <td style="padding:10px;width:25%;vertical-align:top">
              <img src="images/timebalance_cvpr2023.png" alt="timebalance-cvpr2023" width="160" style="border-style: none">
            </td>
            <td style="padding-bottom:35px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2303.16268" id="MCG_journal">
                <papertitle>TimeBalance: Temporally-Invariant and Temporally-Distinctive Video Representations for Semi-Supervised Action Recognition</papertitle>
              </a>
              <br>
              <a href="https://www.linkedin.com/in/ishan-dave-crcv">Ishan Dave</a>, <strong>Mamshad Nayeem Rizve</strong>, <a href="https://www.crcv.ucf.edu/chenchen/">Chen Chen</a>, <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>
              <br>
              <em>CVPR</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2303.16268">arxiv</a>/
              <a href="data/timebalance_cvpr2023.bib">bibtex</a> /
              <a href="https://github.com/DAVEISHAN/TimeBalance">code</a>
              <p>
		   We propose a student-teacher semi-supervised learning framework, where we distill knowledge from a temporally-invariant and temporally-distinctive teacher. Depending on the nature of the unlabeled video, we dynamically combine the knowledge of these two teachers based on a novel temporal similarity-based reweighting scheme.
		</p>
            </td>
          </tr>
          <tr>
            <td style="padding:10px;width:25%;vertical-align:top">
              <img src="images/realistic_eccv2022.png" alt="realistic-eccv2022" width="160" style="border-style: none">
            </td>
            <td style="padding-bottom:35px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2207.02269" id="MCG_journal">
                <papertitle>Towards Realistic Semi-Supervised Learning</papertitle>
              </a>
              <br>
              <strong>Mamshad Nayeem Rizve</strong>, <a href="https://www.linkedin.com/in/navid-kardan-b2630a88/">Navid Kardan</a>, <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>
              <br>
              <em>ECCV</em>, 2022 <font color="red">(Oral presentation; in top 2.7%)</font>
              <br>
              <a href="https://arxiv.org/abs/2207.02269">arxiv</a> /
              <a href="data/realistic_eccv2022.bib">bibtex</a> /
              <a href="https://github.com/nayeemrizve/TRSSL">code</a>
	      <p></p>
              <p>
		We propose a new method for open-world semi-supervised learning that utilizes sample uncertainty and incorporates prior knowledge about class distribution to generate reliable class-distribution-aware pseudo-labels for samples belonging to both known and unknown classes.
	      </p>
		    
            </td>
          </tr>
		
		
          <tr>
            <td style="padding:10px;width:25%;vertical-align:top">
              <img src="images/openldn_eccv2022.png" alt="openldn-eccv2022" width="160" style="border-style: none">
            </td>
            <td style="padding-bottom:35px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2207.02261" id="MCG_journal">
                <papertitle>OpenLDN: Learning to Discover Novel Classes for Open-World Semi-Supervised Learning</papertitle>
              </a>
              <br>
              <strong>Mamshad Nayeem Rizve</strong>, <a href="https://www.linkedin.com/in/navid-kardan-b2630a88/">Navid Kardan</a>, <a href="https://salman-h-khan.github.io/">Salman Khan</a>, <a href="https://sites.google.com/view/fahadkhans/home">Fahad Shahbaz Khan</a>, <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>
              <br>
              <em>ECCV</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2207.02261">arxiv</a> /
              <a href="data/openldn_eccv2022.bib">bibtex</a> /
              <a href="https://github.com/nayeemrizve/OpenLDN">code</a>
              <p>
		    OpenLDN utilizes a pairwise similarity loss with bi-level optimization to discover novel classes and transforms the open-world SSL problem into a standard SSL problem, outperforming current state-of-the-art methods with better accuracy/training time trade-off.
		</p>
            </td>
          </tr>
		
		
          <tr>
            <td style="padding:10px;width:25%;vertical-align:top">
              <img src="images/unicon_cvpr2022.png" alt="invariance-equivariance-cvpr2021" width="160" style="border-style: none">
            </td>
            <td style="padding-bottom:35px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2203.14542" id="MCG_journal">
                <papertitle>UNICON: Combating Label Noise Through Uniform Selection and Contrastive Learning</papertitle>
              </a>
              <br>
              <a href="https://www.linkedin.com/in/nazmul-karim-1b5805115/">Nazmul Karim</a>, <strong>Mamshad Nayeem Rizve</strong>, <a href="https://www.ece.ucf.edu/person/nazanin-rahnavard/">Nazanin Rahnavard</a>, <a href="https://ajmalsaeed.net/">Ajmal Mian</a>, <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>
              <br>
              <em>CVPR</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2203.14542">arxiv</a> /
              <a href="data/unicon_cvpr2022.bib">bibtex</a> /
              <a href="https://github.com/nazmul-karim170/unicon-noisy-label">code</a>
              <p>
		UNICON is a robust sample selection approach for training with high label noise. It incorporates a Jensen-Shannon divergence based uniform sample selection mechanism and contrastive learning.
	</p>
            </td>
          </tr>
          <tr>
            <td style="padding:10px;width:25%;vertical-align:top">
              <img src="images/tclr_cviu2022.png" alt="invariance-equivariance-cvpr2021" width="160" style="border-style: none">
            </td>
            <td style="padding-bottom:35px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2101.07974" id="MCG_journal">
                <papertitle>TCLR: Temporal contrastive learning for video representation</papertitle>
              </a>
              <br>
              <a href="https://www.linkedin.com/in/ishan-dave-crcv">Ishan Dave</a>, <a href="https://www.linkedin.com/in/rohitguptahpf">Rohit Gupta</a>, <strong>Mamshad Nayeem Rizve</strong>, <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>
              <br>
              <em>CVIU</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2101.07974">arxiv</a> /
              <a href="https://www.sciencedirect.com/science/article/pii/S1077314222000376">elsevier</a> /
              <a href="data/tclr_cviu2022.bib">bibtex</a> /
              <a href="https://github.com/DAVEISHAN/TCLR">code</a> /
              <a href="https://www.youtube.com/watch?v=NfE1CXqzE8s&start=2212">video</a>
              <p>We propose a new temporal contrastive learning framework for self-supervised video representation learning, consisting of two novel losses that aim to increase the temporal diversity of learned features. The framework achieves state-of-the-art results on various downstream video understanding tasks, including significant improvement in fine-grained action classification for visually similar classes.</p>
            </td>
          </tr>
          <tr>
            <td style="padding:10px;width:25%;vertical-align:top">
              <img src="images/ier_cvpr2021.png" alt="invariance-equivariance-cvpr2021" width="160" style="border-style: none">
            </td>
            <td style="padding-bottom:35px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2103.01315" id="MCG_journal">
                <papertitle>Exploring Complementary Strengths of Invariant and Equivariant Representations for Few-Shot Learning</papertitle>
              </a>
              <br>
              <strong>Mamshad Nayeem Rizve</strong>, <a href="https://salman-h-khan.github.io/">Salman Khan</a>, <a href="https://sites.google.com/view/fahadkhans/home">Fahad Shahbaz Khan</a>, <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>
              <br>
              <em>CVPR</em>, 2021
              <br>
              <a href="https://arxiv.org/abs/2103.01315">arxiv</a> /
              <a href="data/ier_cvp2021.bib">bibtex</a> /
              <a href="https://github.com/nayeemrizve/invariance-equivariance">code</a> /
              <a href="https://www.youtube.com/watch?v=NfE1CXqzE8s&start=1100">video</a>
              <p>
		    We propose a novel training mechanism for few-shot learning that simultaneously enforces equivariance and invariance to geometric transformations, allowing the model to learn features that generalize well to novel classes with few samples. 
		    </p>
            </td>
          </tr>

          <tr>
            <td style="padding:10px;width:25%;vertical-align:top">
              <img src="images/ups_iclr2021.png" alt="ups-iclr2021" width="160" style="border-style: none">
            </td>
            <td style="padding-bottom:35px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2101.06329" id="MCG_journal">
                <papertitle>In Defense of Pseudo-Labeling: An Uncertainty-Aware Pseudo-label Selection Framework for Semi-Supervised Learning</papertitle>
              </a>
              <br>
              <strong>Mamshad Nayeem Rizve</strong>, <a href="https://www.linkedin.com/in/kevin-duarte-vision/">Kevin Duarte</a>, <a href="https://www.crcv.ucf.edu/person/rawat/">Yogesh S Rawat</a>, <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>
              <br>
              <em>ICLR</em>, 2021
              <br>
              <a href="https://arxiv.org/abs/2101.06329">arxiv</a> /
              <a href="https://openreview.net/forum?id=-ODN6SbiUU">openreview</a> /
              <a href="data/ups_iclr2021.bib">bibtex</a> /
              <a href="https://github.com/nayeemrizve/ups">code</a> /
              <a href="https://www.youtube.com/watch?v=NfE1CXqzE8s&start=20">video</a>
              <p>We propose an uncertainty-aware pseudo-label selection (UPS) framework that reduces pseudo-label noise encountered during training, and allows for the creation of negative pseudo-labels for multi-label classification and negative learning. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:10px;width:25%;vertical-align:top">
              <img src="images/gabriella_icpr2020.png" alt="gabriella-icpr2020" width="160" style="border-style: none">
            </td>
            <td style="padding-bottom:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2004.11475" id="MCG_journal">
                <papertitle>Gabriella: An Online System for Real-Time Activity Detection in Untrimmed Security Videos</papertitle>
              </a>
              <br>
              <strong>Mamshad Nayeem Rizve</strong>, <a href="https://www.linkedin.com/in/demiru/">Ugur Demir</a>, <a href="https://www.linkedin.com/in/praveen-tirupattur-2044ba51/">Praveen Tirupattur</a>, <a href="https://aayushjr.github.io/">Aayush Jung Rana</a>, <a href="https://www.linkedin.com/in/kevin-duarte-vision/">Kevin Duarte</a>, <a href="https://www.linkedin.com/in/ishan-dave-crcv">Ishan Dave</a>, <a href="https://www.crcv.ucf.edu/person/rawat/">Yogesh S Rawat</a>, <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>
              <br>
              <em>ICPR</em>, 2020 <font color="red">(Best paper award)</font>
              <br>
              <a href="https://arxiv.org/abs/2004.11475">arxiv</a> /
              <a href="https://www.crcv.ucf.edu/research/projects/gabriella-an-online-system-for-real-time-activity-detection-in-untrimmed-security-videos/">project page</a> /
              <a href="data/gabriella_icpr2020.bib">bibtex</a> /
              <a href="https://www.crcv.ucf.edu/wp-content/uploads/2020/08/Projects_Gabriella_Slides.pdf">slides</a> /
              <a href="https://www.youtube.com/watch?v=O64331jZczo">video</a>
              <p>Gabbriella consists of three stages: tubelet extraction, activity classification, and online tubelet merging. Gabriella utilizes a localization network for tubelet extraction, with a novel Patch-Dice loss to handle variations in actor size, and a Tubelet-Merge Action-Split (TMAS) algorithm to detect activities efficiently and robustly.</p>
            </td>
          </tr>
					

        </tbody></table>
	      
	      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
             <tr>
             <td style="padding:20px;width:100%;vertical-align:middle"> 
               <heading>Patents</heading>
            </td>
        </tr> 
    </tbody></table>  

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:10px;width:25%;vertical-align:top">
              <img src="images/gabriella_patent.png" alt="gabriella-patent" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://patentimages.storage.googleapis.com/1a/1e/ec/af681095ca1677/US20220222940A1.pdf" id="MCG_journal">
                <papertitle>Methods of Real-Time Spatio-Temporal Activity Detection and Categorization from Untrimmed Video Segments</papertitle>
              </a>
              <br>
              <a href="https://www.crcv.ucf.edu/person/rawat/">Yogesh S Rawat</a>, <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>, <a href="https://aayushjr.github.io/">Aayush Jung Rana</a>, <a href="https://www.linkedin.com/in/praveen-tirupattur-2044ba51/">Praveen Tirupattur</a>, <strong>Mamshad Nayeem Rizve</strong>
              <br>
              US Patent 11468676
              <br>
              <a href="https://patentimages.storage.googleapis.com/1a/1e/ec/af681095ca1677/US20220222940A1.pdf">Details</a>
              <p></p>
            </td>
          </tr>

	</tbody></table> 			
        

  </table>
</body>

</html>
